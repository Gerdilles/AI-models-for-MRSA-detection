{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e6dc0-81c1-4ce8-8234-fa2ab364af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh_ml_bin_variants.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from maldi_nn.spectrum import (\n",
    "    SpectrumObject, SequentialPreprocessor, VarStabilizer, Smoother,\n",
    "    BaselineCorrecter, Trimmer, PersistenceTransformer, Normalizer, Binner\n",
    ")\n",
    "\n",
    "# --- Config ---\n",
    "neg_dir = r\"Y:\\\\test_set\\\\allspectra\\\\neg_spectra\\\\neg_tsv\"\n",
    "pos_dir = r\"Y:\\\\test_set\\\\allspectra\\\\pos_spectra\\\\pos_tsv\"\n",
    "ribo_masslist = r\"Y:\\\\test_set\\\\ribo_Saureus.tsv\"\n",
    "\n",
    "# --- Data Leakage Check ---\n",
    "def check_sample_uniqueness():\n",
    "    all_files = []\n",
    "    for label_dir in [neg_dir, pos_dir]:\n",
    "        if os.path.exists(label_dir):\n",
    "            all_files += [os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith(\".tsv\")]\n",
    "\n",
    "    basenames = [os.path.basename(f) for f in all_files]\n",
    "    duplicates = pd.Series(basenames).duplicated()\n",
    "    if duplicates.any():\n",
    "        print(\"Data leakage detected! Duplicate sample names found across classes:\")\n",
    "        print(pd.Series(basenames)[duplicates].value_counts())\n",
    "    else:\n",
    "        print(f\"No data leakage detected. {len(basenames)} unique sample files.\")\n",
    "\n",
    "check_sample_uniqueness()\n",
    "\n",
    "# --- Load ribosomal m/z values ---\n",
    "try:\n",
    "    ribo_df = pd.read_csv(ribo_masslist, sep=\"\\t\", header=0, comment=\"#\")\n",
    "    ribo_mz = pd.to_numeric(ribo_df['Mass'], errors='coerce').dropna().values if 'Mass' in ribo_df.columns else np.array([])\n",
    "    print(f\"Loaded {len(ribo_mz)} ribosomal m/z values.\")\n",
    "except:\n",
    "    ribo_mz = np.array([])\n",
    "    print(\"Failed to load ribosomal mass list. Proceeding without alignment.\")\n",
    "\n",
    "# --- Alignment ---\n",
    "def align_spectrum(spectrum, ref_mz_list, top_n=10, ppm=100):\n",
    "    if not spectrum or len(ref_mz_list) == 0: return spectrum\n",
    "    current_mz = np.asarray(spectrum.mz)\n",
    "    current_intensity = np.asarray(spectrum.intensity)\n",
    "    idx = np.argsort(-current_intensity)[:top_n]\n",
    "    best_shift = 0.0\n",
    "    min_error = float('inf')\n",
    "    for peak in current_mz[idx]:\n",
    "        delta = ref_mz_list - peak\n",
    "        mask = np.abs(delta) <= (peak * ppm / 1e6)\n",
    "        if np.any(mask):\n",
    "            shift = delta[mask][np.argmin(np.abs(delta[mask]))]\n",
    "            if abs(shift) < min_error:\n",
    "                best_shift, min_error = shift, abs(shift)\n",
    "    spectrum.mz = current_mz + best_shift\n",
    "    return spectrum\n",
    "\n",
    "# --- Preprocessing Steps ---\n",
    "preproc_variants = [True, False]\n",
    "bin_sizes = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Random Forest variants\n",
    "rf_variants = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=300, max_depth=15, random_state=42),\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "]\n",
    "\n",
    "# SVM variants (with scaling)\n",
    "svm_variants = [\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=0.1, kernel=\"linear\", probability=True, random_state=42))]),\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=1, kernel=\"linear\", probability=True, random_state=42))]),\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=10, kernel=\"linear\", probability=True, random_state=42))]),\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=1, kernel=\"rbf\", probability=True, random_state=42))]),\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=10, kernel=\"rbf\", probability=True, random_state=42))])\n",
    "]\n",
    "\n",
    "# LightGBM variants\n",
    "lgbm_variants = [\n",
    "    LGBMClassifier(n_estimators=100, learning_rate=0.05, random_state=42, verbosity=-1),\n",
    "    LGBMClassifier(n_estimators=200, learning_rate=0.1, random_state=42, verbosity=-1),\n",
    "    LGBMClassifier(n_estimators=200, learning_rate=0.2, random_state=42, verbosity=-1),\n",
    "    LGBMClassifier(n_estimators=300, learning_rate=0.05, random_state=42, verbosity=-1),\n",
    "    LGBMClassifier(n_estimators=100, learning_rate=0.01, random_state=42, verbosity=-1)\n",
    "]\n",
    "\n",
    "# XGBoost variants\n",
    "xgb_variants = [\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=200, learning_rate=0.1, random_state=42),\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=200, learning_rate=0.2, random_state=42),\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=300, learning_rate=0.05, random_state=42),\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100, learning_rate=0.01, random_state=42)\n",
    "]\n",
    "\n",
    "# Logistic Regression variants (with scaling)\n",
    "logreg_variants = [\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=0.01, max_iter=1000, random_state=42))]),\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=0.1, max_iter=1000, random_state=42))]),\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=1, max_iter=1000, random_state=42))]),\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=10, max_iter=1000, random_state=42))]),\n",
    "    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=100, max_iter=1000, random_state=42))])\n",
    "]\n",
    "\n",
    "summary = []\n",
    "metrics_detailed = []\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for bin_step in bin_sizes:\n",
    "    for use_persistence in preproc_variants:\n",
    "        print(f\"\\n Processing bin size {bin_step}, persistence={use_persistence}...\")\n",
    "        preproc = SequentialPreprocessor(\n",
    "            VarStabilizer(\"sqrt\"),\n",
    "            Smoother(10),\n",
    "            BaselineCorrecter(\"SNIP\", 20),\n",
    "            Trimmer(),\n",
    "            PersistenceTransformer(use_persistence),\n",
    "            Normalizer(1)\n",
    "        )\n",
    "        binner = Binner(2000, 15000, bin_step)\n",
    "        X, y = [], []\n",
    "\n",
    "        for folder, label in [(neg_dir, 0), (pos_dir, 1)]:\n",
    "            if not os.path.exists(folder): continue\n",
    "            for fname in os.listdir(folder):\n",
    "                if not fname.endswith(\".tsv\") or not fname.startswith(\"2024\"): continue\n",
    "                path = os.path.join(folder, fname)\n",
    "                try:\n",
    "                    spec = SpectrumObject.from_tsv(path, sep=\"\\t\")\n",
    "                    spec = preproc(spec)\n",
    "                    spec = align_spectrum(spec, ribo_mz, ppm=100)\n",
    "                    spec = binner(spec)\n",
    "                    if len(spec.intensity):\n",
    "                        X.append(np.asarray(spec.intensity).flatten())\n",
    "                        y.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipped {fname}: {e}\")\n",
    "\n",
    "        if len(X) < 10:\n",
    "            print(\"Not enough data, skipping.\")\n",
    "            continue\n",
    "\n",
    "        X, y = np.stack(X), np.array(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        def run_model(model, name):\n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                probas = model.predict_proba(X_test)[:, 1]\n",
    "                preds = model.predict(X_test)\n",
    "                auc = roc_auc_score(y_test, probas)\n",
    "                fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "                plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "                report = classification_report(y_test, preds, output_dict=True)\n",
    "                cmatrix = confusion_matrix(y_test, preds).tolist()\n",
    "                summary.append({\"variant\": name, \"auc\": auc})\n",
    "                metrics_detailed.append({\"variant\": name, \"auc\": auc, \"report\": report, \"confusion_matrix\": cmatrix})\n",
    "                print(f\"{name} AUC={auc:.3f}\")\n",
    "                print(pd.DataFrame(report))\n",
    "                print(f\"Confusion Matrix:\\n{cmatrix}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{name} failed: {e}\")\n",
    "                metrics_detailed.append({\"variant\": name, \"auc\": 0.0, \"error\": str(e)})\n",
    "\n",
    "        for i, model in enumerate(rf_variants):\n",
    "            run_model(model, f\"RF_bin{bin_step}_pers{use_persistence}_v{i}\")\n",
    "        for i, model in enumerate(svm_variants):\n",
    "            run_model(model, f\"SVM_bin{bin_step}_pers{use_persistence}_v{i}\")\n",
    "        for i, model in enumerate(lgbm_variants):\n",
    "            run_model(model, f\"LGBM_bin{bin_step}_pers{use_persistence}_v{i}\")\n",
    "        for i, model in enumerate(xgb_variants):\n",
    "            run_model(model, f\"XGB_bin{bin_step}_pers{use_persistence}_v{i}\")\n",
    "        for i, model in enumerate(logreg_variants):\n",
    "            run_model(model, f\"LOGREG_bin{bin_step}_pers{use_persistence}_v{i}\")\n",
    "\n",
    "        try:\n",
    "            print(f\"Running KMeans_bin{bin_step}_pers{use_persistence}\")\n",
    "            km = KMeans(n_clusters=2, random_state=42, n_init='auto').fit(X_train)\n",
    "            preds = km.predict(X_test)\n",
    "            if np.mean(y_test[preds == 0]) > np.mean(y_test[preds == 1]):\n",
    "                preds = 1 - preds\n",
    "            auc = roc_auc_score(y_test, preds)\n",
    "            fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "            plt.plot(fpr, tpr, label=f\"KMeans_bin{bin_step}_pers{use_persistence} (AUC={auc:.3f})\")\n",
    "            report = classification_report(y_test, preds, output_dict=True)\n",
    "            cmatrix = confusion_matrix(y_test, preds).tolist()\n",
    "            summary.append({\"variant\": f\"KMeans_bin{bin_step}_pers{use_persistence}\", \"auc\": auc})\n",
    "            metrics_detailed.append({\"variant\": f\"KMeans_bin{bin_step}_pers{use_persistence}\", \"auc\": auc, \"report\": report, \"confusion_matrix\": cmatrix})\n",
    "            print(f\"KMeans_bin{bin_step}_pers{use_persistence} AUC={auc:.3f}\")\n",
    "            print(pd.DataFrame(report))\n",
    "            print(f\"Confusion Matrix:\\n{cmatrix}\")\n",
    "        except Exception as e:\n",
    "            print(f\"KMeans failed: {e}\")\n",
    "            metrics_detailed.append({\"variant\": f\"KMeans_bin{bin_step}_pers{use_persistence}\", \"auc\": 0.0, \"error\": str(e)})\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.legend(fontsize='x-small', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout(rect=[0, 0, 0.75, 1])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for Bins 1-5 (2024 data only)\")\n",
    "plt.savefig(\"roc_bins_2024.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "pd.DataFrame(summary).to_csv(\"auc_summary_bins_2024.csv\", index=False)\n",
    "pd.DataFrame(metrics_detailed).to_json(\"ml_metrics_bins_2024_detailed.json\", orient=\"records\", indent=2)\n",
    "print(\"\\n Done: Summary written to auc_summary_bins_2024.csv, full metrics in ml_metrics_bins_2024_detailed.json, and ROC to roc_bins_2024.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c09ca2-955e-4496-9fd8-1c6a50b4114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh_ml_bin_variants.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from maldi_nn.spectrum import (\n",
    "    SpectrumObject, SequentialPreprocessor, VarStabilizer, Smoother,\n",
    "    BaselineCorrecter, Trimmer, PersistenceTransformer, Normalizer, Binner\n",
    ")\n",
    "\n",
    "# --- Config ---\n",
    "neg_dir = r\"Y:\\\\test_set\\\\allspectra\\\\neg_spectra\\\\neg_tsv\"\n",
    "pos_dir = r\"Y:\\\\test_set\\\\allspectra\\\\pos_spectra\\\\pos_tsv\"\n",
    "ribo_masslist = r\"Y:\\\\test_set\\\\ribo_Saureus.tsv\"\n",
    "BIN_START_MZ = 2000\n",
    "BIN_END_MZ = 15000\n",
    "\n",
    "# --- Data Leakage Check ---\n",
    "def check_sample_uniqueness():\n",
    "    all_files = []\n",
    "    for label_dir in [neg_dir, pos_dir]:\n",
    "        if os.path.exists(label_dir):\n",
    "            all_files += [os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith(\".tsv\")]\n",
    "\n",
    "    basenames = [os.path.basename(f) for f in all_files]\n",
    "    duplicates = pd.Series(basenames).duplicated()\n",
    "    if duplicates.any():\n",
    "        print(\"Data leakage detected! Duplicate sample names found across classes:\")\n",
    "        print(pd.Series(basenames)[duplicates].value_counts())\n",
    "    else:\n",
    "        print(f\"No data leakage detected. {len(basenames)} unique sample files.\")\n",
    "\n",
    "check_sample_uniqueness()\n",
    "\n",
    "# --- Load ribosomal m/z values ---\n",
    "try:\n",
    "    ribo_df = pd.read_csv(ribo_masslist, sep=\"\\t\", header=0, comment=\"#\")\n",
    "    ribo_mz = pd.to_numeric(ribo_df['Mass'], errors='coerce').dropna().values if 'Mass' in ribo_df.columns else np.array([])\n",
    "    print(f\"Loaded {len(ribo_mz)} ribosomal m/z values.\")\n",
    "except:\n",
    "    ribo_mz = np.array([])\n",
    "    print(\"Failed to load ribosomal mass list. Proceeding without alignment.\")\n",
    "\n",
    "# --- Alignment ---\n",
    "def align_spectrum(spectrum, ref_mz_list, top_n=10, ppm=100):\n",
    "    if not hasattr(spectrum, 'mz') or not hasattr(spectrum, 'intensity') or len(ref_mz_list) == 0:\n",
    "        return spectrum\n",
    "    current_mz = np.asarray(spectrum.mz)\n",
    "    current_intensity = np.asarray(spectrum.intensity)\n",
    "    if len(current_mz) == 0:\n",
    "        return spectrum\n",
    "    idx = np.argsort(-current_intensity)[:top_n]\n",
    "    best_shift = 0.0\n",
    "    min_error = float('inf')\n",
    "    for peak in current_mz[idx]:\n",
    "        delta = ref_mz_list - peak\n",
    "        mask = np.abs(delta) <= (peak * ppm / 1e6)\n",
    "        if np.any(mask):\n",
    "            shift = delta[mask][np.argmin(np.abs(delta[mask]))]\n",
    "            if abs(shift) < min_error:\n",
    "                best_shift, min_error = shift, abs(shift)\n",
    "    spectrum.mz = current_mz + best_shift\n",
    "    return spectrum\n",
    "\n",
    "# --- Preprocessing Steps ---\n",
    "preproc_variants = [True, False]\n",
    "bin_sizes = [1, 2, 3, 4, 5]\n",
    "\n",
    "# --- Model Definitions ---\n",
    "# Random Forest variants\n",
    "rf_variants = [RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42), RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42), RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42), RandomForestClassifier(n_estimators=300, max_depth=15, random_state=42), RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)]\n",
    "# SVM variants (with scaling)\n",
    "svm_variants = [Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=0.1, kernel=\"linear\", probability=True, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=1, kernel=\"linear\", probability=True, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=10, kernel=\"linear\", probability=True, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=1, kernel=\"rbf\", probability=True, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=10, kernel=\"rbf\", probability=True, random_state=42))])]\n",
    "# LightGBM variants\n",
    "lgbm_variants = [LGBMClassifier(n_estimators=100, learning_rate=0.05, random_state=42, verbosity=-1), LGBMClassifier(n_estimators=200, learning_rate=0.1, random_state=42, verbosity=-1), LGBMClassifier(n_estimators=200, learning_rate=0.2, random_state=42, verbosity=-1), LGBMClassifier(n_estimators=300, learning_rate=0.05, random_state=42, verbosity=-1), LGBMClassifier(n_estimators=100, learning_rate=0.01, random_state=42, verbosity=-1)]\n",
    "# XGBoost variants\n",
    "xgb_variants = [XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100, learning_rate=0.1, random_state=42), XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=200, learning_rate=0.1, random_state=42), XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=200, learning_rate=0.2, random_state=42), XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=300, learning_rate=0.05, random_state=42), XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100, learning_rate=0.01, random_state=42)]\n",
    "# Logistic Regression variants (with scaling)\n",
    "logreg_variants = [Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=0.01, max_iter=1000, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=0.1, max_iter=1000, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=1, max_iter=1000, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=10, max_iter=1000, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=100, max_iter=1000, random_state=42))])]\n",
    "\n",
    "summary = []\n",
    "metrics_detailed = []\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for bin_step in bin_sizes:\n",
    "    for use_persistence in preproc_variants:\n",
    "        print(f\"\\n Processing bin size {bin_step}, persistence={use_persistence}...\")\n",
    "        preproc = SequentialPreprocessor(\n",
    "            VarStabilizer(\"sqrt\"),\n",
    "            Smoother(10),\n",
    "            BaselineCorrecter(\"SNIP\", 20),\n",
    "            Trimmer(),\n",
    "            PersistenceTransformer(use_persistence),\n",
    "            Normalizer(1)\n",
    "        )\n",
    "        binner = Binner(BIN_START_MZ, BIN_END_MZ, bin_step)\n",
    "        X, y = [], []\n",
    "\n",
    "        for folder, label in [(neg_dir, 0), (pos_dir, 1)]:\n",
    "            if not os.path.exists(folder): continue\n",
    "            for fname in os.listdir(folder):\n",
    "                if not fname.endswith(\".tsv\") or not fname.startswith(\"2024\"): continue\n",
    "                path = os.path.join(folder, fname)\n",
    "                try:\n",
    "                    spec = SpectrumObject.from_tsv(path, sep=\"\\t\")\n",
    "                    spec = preproc(spec)\n",
    "                    spec = align_spectrum(spec, ribo_mz, ppm=100)\n",
    "                    spec = binner(spec)\n",
    "                    if len(spec.intensity):\n",
    "                        X.append(np.asarray(spec.intensity).flatten())\n",
    "                        y.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipped {fname}: {e}\")\n",
    "\n",
    "        if len(X) < 10:\n",
    "            print(\"Not enough data, skipping.\")\n",
    "            continue\n",
    "\n",
    "        X, y = np.stack(X), np.array(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # --- NEW: Generate m/z range feature names ---\n",
    "        mz_feature_names = [f\"{int(BIN_START_MZ + i * bin_step)}-{int(BIN_START_MZ + (i + 1) * bin_step)}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "        # --- UPDATED: run_model function now accepts feature_names ---\n",
    "        def run_model(model, name, feature_names):\n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                probas = model.predict_proba(X_test)[:, 1]\n",
    "                preds = model.predict(X_test)\n",
    "                auc = roc_auc_score(y_test, probas)\n",
    "                fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "                plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "                report = classification_report(y_test, preds, output_dict=True)\n",
    "                cmatrix = confusion_matrix(y_test, preds).tolist()\n",
    "                summary.append({\"variant\": name, \"auc\": auc})\n",
    "                metrics_detailed.append({\"variant\": name, \"auc\": auc, \"report\": report, \"confusion_matrix\": cmatrix})\n",
    "                print(f\"{name} AUC={auc:.3f}\")\n",
    "                print(pd.DataFrame(report))\n",
    "                print(f\"Confusion Matrix:\\n{cmatrix}\")\n",
    "\n",
    "                try:\n",
    "                    if hasattr(model, \"predict_proba\") or hasattr(model, \"decision_function\"):\n",
    "                        # UPDATED: Use the passed feature_names list\n",
    "                        if not isinstance(model, (RandomForestClassifier, LGBMClassifier, XGBClassifier)):\n",
    "                            print(\"Using summarized background data for SHAP (non-tree model)...\")\n",
    "                            background_data = shap.kmeans(X_train, 100)\n",
    "                            explainer = shap.Explainer(model, background_data, feature_names=feature_names)\n",
    "                        else:\n",
    "                            explainer = shap.Explainer(model, X_train, feature_names=feature_names)\n",
    "\n",
    "                        shap_values = explainer(X_test)\n",
    "                        shap_values_class_1 = shap_values[:,:,1]\n",
    "\n",
    "                        bees_path = f\"shap_beeswarm_{name}.png\"\n",
    "                        water_path = f\"shap_waterfall_{name}.png\"\n",
    "\n",
    "                        plt.figure()\n",
    "                        # UPDATED: max_display is now 25\n",
    "                        shap.plots.beeswarm(shap_values_class_1, max_display=25, show=False)\n",
    "                        plt.title(f\"SHAP Beeswarm - {name}\")\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(bees_path)\n",
    "                        plt.close()\n",
    "\n",
    "                        plt.figure()\n",
    "                        shap.plots.waterfall(shap_values_class_1[0], show=False)\n",
    "                        plt.title(f\"SHAP Waterfall (1st sample) - {name}\")\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(water_path)\n",
    "                        plt.close()\n",
    "\n",
    "                        print(f\"SHAP plots saved: {bees_path}, {water_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ SHAP failed for {name}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"{name} failed: {e}\")\n",
    "                metrics_detailed.append({\"variant\": name, \"auc\": 0.0, \"error\": str(e)})\n",
    "        \n",
    "        # --- UPDATED: Pass mz_feature_names to each model run ---\n",
    "        for i, model in enumerate(rf_variants):\n",
    "            run_model(model, f\"RF_bin{bin_step}_pers{use_persistence}_v{i}\", mz_feature_names)\n",
    "        for i, model in enumerate(svm_variants):\n",
    "            run_model(model, f\"SVM_bin{bin_step}_pers{use_persistence}_v{i}\", mz_feature_names)\n",
    "        for i, model in enumerate(lgbm_variants):\n",
    "            run_model(model, f\"LGBM_bin{bin_step}_pers{use_persistence}_v{i}\", mz_feature_names)\n",
    "        for i, model in enumerate(xgb_variants):\n",
    "            run_model(model, f\"XGB_bin{bin_step}_pers{use_persistence}_v{i}\", mz_feature_names)\n",
    "        for i, model in enumerate(logreg_variants):\n",
    "            run_model(model, f\"LOGREG_bin{bin_step}_pers{use_persistence}_v{i}\", mz_feature_names)\n",
    "\n",
    "        try:\n",
    "            print(f\"Running KMeans_bin{bin_step}_pers{use_persistence}\")\n",
    "            km = KMeans(n_clusters=2, random_state=42, n_init='auto').fit(X_train)\n",
    "            preds = km.predict(X_test)\n",
    "            if np.mean(y_test[preds == 0]) > np.mean(y_test[preds == 1]): preds = 1 - preds\n",
    "            auc = roc_auc_score(y_test, preds)\n",
    "            fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "            plt.plot(fpr, tpr, label=f\"KMeans_bin{bin_step}_pers{use_persistence} (AUC={auc:.3f})\")\n",
    "            report = classification_report(y_test, preds, output_dict=True)\n",
    "            cmatrix = confusion_matrix(y_test, preds).tolist()\n",
    "            summary.append({\"variant\": f\"KMeans_bin{bin_step}_pers{use_persistence}\", \"auc\": auc})\n",
    "            metrics_detailed.append({\"variant\": f\"KMeans_bin{bin_step}_pers{use_persistence}\", \"auc\": auc, \"report\": report, \"confusion_matrix\": cmatrix})\n",
    "            print(f\"KMeans_bin{bin_step}_pers{use_persistence} AUC={auc:.3f}\")\n",
    "            print(pd.DataFrame(report))\n",
    "            print(f\"Confusion Matrix:\\n{cmatrix}\")\n",
    "        except Exception as e:\n",
    "            print(f\"KMeans failed: {e}\")\n",
    "            metrics_detailed.append({\"variant\": f\"KMeans_bin{bin_step}_pers{use_persistence}\", \"auc\": 0.0, \"error\": str(e)})\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.legend(fontsize='x-small', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout(rect=[0, 0, 0.75, 1])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for Bins 1-5 (2024 data only)\")\n",
    "plt.savefig(\"roc_bins_2024.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "pd.DataFrame(summary).to_csv(\"auc_summary_bins_2024.csv\", index=False)\n",
    "pd.DataFrame(metrics_detailed).to_json(\"ml_metrics_bins_2024_detailed.json\", orient=\"records\", indent=2)\n",
    "print(\"\\n Done: Summary written to auc_summary_bins_2024.csv, full metrics in ml_metrics_bins_2024_detailed.json, and ROC to roc_bins_2024.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44c030-cea5-4dd8-aec0-c7916270da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh_ml_bin_variants.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import shap\n",
    "from shap import KernelExplainer\n",
    "import warnings\n",
    "from sklearn.base import clone\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from maldi_nn.spectrum import (\n",
    "    SpectrumObject, SequentialPreprocessor, VarStabilizer, Smoother,\n",
    "    BaselineCorrecter, Trimmer, PersistenceTransformer, Normalizer, Binner\n",
    ")\n",
    "\n",
    "# --- Config ---\n",
    "neg_dir = r\"Y:\\\\test_set\\\\allspectra\\\\neg_spectra\\\\neg_tsv\"\n",
    "pos_dir = r\"Y:\\\\test_set\\\\allspectra\\\\pos_spectra\\\\pos_tsv\"\n",
    "ribo_masslist = r\"Y:\\\\test_set\\\\ribo_Saureus.tsv\"\n",
    "BIN_START_MZ = 2000\n",
    "BIN_END_MZ = 15000\n",
    "ALL_YEARS = ['2022', '2023', '2024']\n",
    "\n",
    "# --- RANSAC Aligner Class ---\n",
    "class Aligner:\n",
    "    \"\"\"A preprocessor step to align a spectrum's m/z axis to a reference peak list.\"\"\"\n",
    "    def __init__(self, reference_peaks, tolerance=500):\n",
    "        self.reference_peaks = np.array(reference_peaks)\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def __call__(self, spectrum: SpectrumObject) -> SpectrumObject:\n",
    "        peaks, _ = find_peaks(spectrum.intensity, height=0.05 * np.max(spectrum.intensity), prominence=0.01)\n",
    "        if len(peaks) < 5: return spectrum\n",
    "        sample_peak_mzs = spectrum.mz[peaks]\n",
    "        shifts = []\n",
    "        for ref_peak in self.reference_peaks:\n",
    "            tol_daltons = self.tolerance * ref_peak / 1e6\n",
    "            matches = np.where(np.abs(sample_peak_mzs - ref_peak) < tol_daltons)[0]\n",
    "            if len(matches) > 0:\n",
    "                best_match_idx = matches[np.argmin(np.abs(sample_peak_mzs[matches] - ref_peak))]\n",
    "                sample_peak_mz = sample_peak_mzs[best_match_idx]\n",
    "                shifts.append((sample_peak_mz, ref_peak - sample_peak_mz))\n",
    "        if len(shifts) < 3: return spectrum\n",
    "        shifts = np.array(shifts)\n",
    "        sample_mzs_for_fit, mz_shifts_for_fit = shifts[:, 0].reshape(-1, 1), shifts[:, 1]\n",
    "        try:\n",
    "            ransac = RANSACRegressor(random_state=42).fit(sample_mzs_for_fit, mz_shifts_for_fit)\n",
    "            mz_correction = ransac.predict(spectrum.mz.reshape(-1, 1))\n",
    "            return SpectrumObject(spectrum.mz + mz_correction, spectrum.intensity)\n",
    "        except ValueError:\n",
    "            return spectrum\n",
    "\n",
    "# --- Data Leakage Check ---\n",
    "def check_sample_uniqueness():\n",
    "    all_files = []\n",
    "    for label_dir in [neg_dir, pos_dir]:\n",
    "        if os.path.exists(label_dir):\n",
    "            all_files += [os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith(\".tsv\")]\n",
    "    basenames = [os.path.basename(f) for f in all_files]\n",
    "    if pd.Series(basenames).duplicated().any():\n",
    "        print(\"Data leakage detected!\")\n",
    "    else:\n",
    "        print(f\"No data leakage detected. {len(basenames)} unique sample files.\")\n",
    "check_sample_uniqueness()\n",
    "\n",
    "# --- Reference Peak Loading ---\n",
    "try:\n",
    "    ref_df = pd.read_csv(ribo_masslist, sep='\\t', engine='python')\n",
    "    proton_mass = 1.007276\n",
    "    reference_peaks = (ref_df['Mass'] + proton_mass).values\n",
    "    print(f\"Loaded and processed {len(reference_peaks)} reference peaks for alignment.\")\n",
    "except Exception as e:\n",
    "    reference_peaks = np.array([])\n",
    "    print(f\"Failed to load reference mass list, proceeding without alignment: {e}\")\n",
    "\n",
    "# --- RESTORED: Experiment parameters ---\n",
    "preproc_variants = [True, False]\n",
    "bin_sizes = [1, 2, 3, 4, 5]\n",
    "\n",
    "# --- Model Definitions (SVM Last) ---\n",
    "model_definitions = {\n",
    "    \"RF\": [RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1), RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1), RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, n_jobs=-1), RandomForestClassifier(n_estimators=300, max_depth=15, random_state=42, n_jobs=-1), RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1)],\n",
    "    \"LGBM\": [LGBMClassifier(n_estimators=100, learning_rate=0.05, random_state=42, verbosity=-1, n_jobs=-1, device='gpu'), LGBMClassifier(n_estimators=200, learning_rate=0.1, random_state=42, verbosity=-1, n_jobs=-1, device='gpu'), LGBMClassifier(n_estimators=200, learning_rate=0.2, random_state=42, verbosity=-1, n_jobs=-1, device='gpu'), LGBMClassifier(n_estimators=300, learning_rate=0.05, random_state=42, verbosity=-1, n_jobs=-1, device='gpu'), LGBMClassifier(n_estimators=100, learning_rate=0.01, random_state=42, verbosity=-1, n_jobs=-1, device='gpu')],\n",
    "    \"XGB\": [XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1, device='cuda', tree_method='hist'), XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=200, learning_rate=0.1, random_state=42, n_jobs=-1, device='cuda', tree_method='hist'), XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=200, learning_rate=0.2, random_state=42, n_jobs=-1, device='cuda', tree_method='hist'), XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=300, learning_rate=0.05, random_state=42, n_jobs=-1, device='cuda', tree_method='hist'), XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100, learning_rate=0.01, random_state=42, n_jobs=-1, device='cuda', tree_method='hist')],\n",
    "    \"LOGREG\": [Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=0.01, max_iter=1000, random_state=42, n_jobs=-1))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=0.1, max_iter=1000, random_state=42, n_jobs=-1))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=1, max_iter=1000, random_state=42, n_jobs=-1))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=10, max_iter=1000, random_state=42, n_jobs=-1))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(C=100, max_iter=1000, random_state=42, n_jobs=-1))])],\n",
    "    \"SVM\": [Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=0.1, kernel=\"linear\", probability=True, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=1, kernel=\"linear\", probability=True, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=10, kernel=\"linear\", probability=True, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=1, kernel=\"rbf\", probability=True, random_state=42))]), Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(C=10, kernel=\"rbf\", probability=True, random_state=42))])]\n",
    "}\n",
    "summary, metrics_detailed = [], []\n",
    "\n",
    "def evaluate_performance(model, name, feature_names, X_train, X_test, y_test):\n",
    "    try:\n",
    "        probas = model.predict_proba(X_test)[:, 1]\n",
    "        preds = model.predict(X_test)\n",
    "        auc = roc_auc_score(y_test, probas)\n",
    "        fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "        report = classification_report(y_test, preds, output_dict=True, zero_division=0)\n",
    "        cmatrix = confusion_matrix(y_test, preds).tolist()\n",
    "        summary.append({\"variant\": name, \"auc\": auc})\n",
    "        metrics_detailed.append({\"variant\": name, \"auc\": auc, \"report\": report, \"confusion_matrix\": cmatrix})\n",
    "        print(f\"{name} AUC={auc:.3f}\")\n",
    "        \n",
    "        try:\n",
    "            is_tree_model = isinstance(model, (RandomForestClassifier, LGBMClassifier, XGBClassifier))\n",
    "            \n",
    "            if is_tree_model:\n",
    "                print(\"Using fast TreeExplainer...\")\n",
    "                X_test_subset = shap.sample(X_test, 100, random_state=42)\n",
    "                explainer = shap.Explainer(model, X_train)\n",
    "                shap_explanation = explainer(X_test_subset)\n",
    "\n",
    "                if shap_explanation.values.ndim == 3:\n",
    "                    shap_explanation = shap_explanation[:,:,1]\n",
    "                \n",
    "                shap_explanation.feature_names = feature_names\n",
    "                \n",
    "                plt.figure() \n",
    "                shap.plots.beeswarm(shap_explanation, max_display=25, show=False)\n",
    "                plt.title(f\"SHAP Beeswarm - {name}\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"shap_beeswarm_{name}.png\")\n",
    "                plt.close()\n",
    "            else:\n",
    "                model_type = \"Non-Tree Model\"\n",
    "                if isinstance(model, Pipeline):\n",
    "                    model_type = model.named_steps['clf'].__class__.__name__\n",
    "                print(f\"Skipping SHAP for slow model type: {model_type}\")\n",
    "\n",
    "        except Exception as e: \n",
    "            print(f\"SHAP failed for {name}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed for {name}: {e}\")\n",
    "        metrics_detailed.append({\"variant\": name, \"auc\": 0.0, \"error\": str(e)})\n",
    "\n",
    "def process_file(filepath, label, preproc, aligner, binner):\n",
    "    \"\"\"Loads and preprocesses a single spectrum file using the Aligner class.\"\"\"\n",
    "    try:\n",
    "        spec = SpectrumObject.from_tsv(filepath, sep=\"\\t\")\n",
    "        spec = preproc(spec)\n",
    "        spec = aligner(spec)\n",
    "        spec = binner(spec)\n",
    "        if len(spec.intensity) > 0: return (np.asarray(spec.intensity).flatten(), label)\n",
    "    except Exception as e: print(f\"Skipped {os.path.basename(filepath)}: {e}\")\n",
    "    return None\n",
    "\n",
    "# --- Main processing loop ---\n",
    "for bin_step in bin_sizes:\n",
    "    for use_persistence in preproc_variants:\n",
    "        print(f\"\\n==========================================================\")\n",
    "        print(f\"Processing Bin Size: {bin_step}, Persistence: {use_persistence}\")\n",
    "        print(f\"==========================================================\")\n",
    "        \n",
    "        preproc = SequentialPreprocessor(VarStabilizer(\"sqrt\"), Smoother(10), BaselineCorrecter(\"SNIP\", 20), Trimmer(), PersistenceTransformer(use_persistence), Normalizer(1))\n",
    "        binner = Binner(BIN_START_MZ, BIN_END_MZ, bin_step)\n",
    "        aligner = Aligner(reference_peaks=reference_peaks, tolerance=500)\n",
    "        \n",
    "        all_files_to_process = [{'filepath': os.path.join(folder, fname), 'label': label} for folder, label in [(neg_dir, 0), (pos_dir, 1)] if os.path.exists(folder) for fname in os.listdir(folder) if fname.endswith(\".tsv\")]\n",
    "        print(f\"Found {len(all_files_to_process)} total files. Starting parallel preprocessing...\")\n",
    "        \n",
    "        results = Parallel(n_jobs=-1, verbose=10)(delayed(process_file)(f['filepath'], f['label'], preproc, aligner, binner) for f in all_files_to_process)\n",
    "        \n",
    "        data_by_year = {year: [] for year in ALL_YEARS}\n",
    "        labels_by_year = {year: [] for year in ALL_YEARS}\n",
    "        for i, res in enumerate(results):\n",
    "            if res:\n",
    "                fname = os.path.basename(all_files_to_process[i]['filepath'])\n",
    "                file_year = next((year for year in ALL_YEARS if fname.startswith(year)), None)\n",
    "                if file_year:\n",
    "                    data_by_year[file_year].append(res[0])\n",
    "                    labels_by_year[file_year].append(res[1])\n",
    "\n",
    "        datasets = {}\n",
    "        for year in ALL_YEARS:\n",
    "            if len(data_by_year[year]) > 10:\n",
    "                datasets[year] = (np.stack(data_by_year[year]), np.array(labels_by_year[year]))\n",
    "                print(f\"Loaded {len(datasets[year][1])} samples for {year}.\")\n",
    "            else:\n",
    "                print(f\"Not enough data for {year}, skipping this year in tests.\")\n",
    "\n",
    "        if not datasets:\n",
    "            print(\"No years with sufficient data. Skipping this configuration.\")\n",
    "            continue\n",
    "            \n",
    "        scenarios = [\n",
    "            {'train': ['2022'], 'test': ['2022', '2023', '2024']},\n",
    "            {'train': ['2023'], 'test': ['2022', '2023', '2024']},\n",
    "            {'train': ['2024'], 'test': ['2022', '2023', '2024']},\n",
    "            {'train': ['2022', '2023'], 'test': ['2024']},\n",
    "            {'train': ['All'], 'test': ['All']}\n",
    "        ]\n",
    "\n",
    "        for scenario in scenarios:\n",
    "            train_years, test_years = scenario['train'], scenario['test']\n",
    "            if 'All' in train_years: train_years_str = \"All\"\n",
    "            else: train_years_str = \"+\".join(train_years)\n",
    "            print(f\"\\n\\n--- Starting Scenario: TRAIN on {train_years_str} ---\")\n",
    "\n",
    "            if train_years_str == \"All\":\n",
    "                if not datasets: continue\n",
    "                all_X, all_y = np.concatenate([d[0] for d in datasets.values()]), np.concatenate([d[1] for d in datasets.values()])\n",
    "                X_train, X_test_split, y_train, y_test_split = train_test_split(all_X, all_y, test_size=0.2, random_state=42, stratify=all_y)\n",
    "                test_sets = {f\"{train_years_str}_held_out\": (X_test_split, y_test_split)}\n",
    "            else:\n",
    "                X_train_list, y_train_list, test_sets = [], [], {}\n",
    "                for year in train_years:\n",
    "                    if year in datasets and year in test_years:\n",
    "                        print(f\"Splitting {year} for training and testing.\")\n",
    "                        X_year, y_year = datasets[year]\n",
    "                        X_train_part, X_test_part, y_train_part, y_test_part = train_test_split(X_year, y_year, test_size=0.2, random_state=42, stratify=y_year)\n",
    "                        X_train_list.append(X_train_part); y_train_list.append(y_train_part); test_sets[year] = (X_test_part, y_test_part)\n",
    "                    elif year in datasets:\n",
    "                        X_train_list.append(datasets[year][0]); y_train_list.append(datasets[year][1])\n",
    "                for year in test_years:\n",
    "                    if year not in train_years and year in datasets: test_sets[year] = datasets[year]\n",
    "                if not X_train_list:\n",
    "                    print(f\"Skipping scenario for train years {train_years_str} due to missing data.\")\n",
    "                    continue\n",
    "                X_train, y_train = np.concatenate(X_train_list), np.concatenate(y_train_list)\n",
    "\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            mz_feature_names = [f\"{int(BIN_START_MZ + i * bin_step)}-{int(BIN_START_MZ + (i + 1) * bin_step)}\" for i in range(X_train.shape[1])]\n",
    "            \n",
    "            for model_type, model_variants in model_definitions.items():\n",
    "                for i, model_template in enumerate(model_variants):\n",
    "                    model = clone(model_template)\n",
    "                    print(f\"\\n-- Fitting {model_type}_v{i} on {train_years_str} data --\")\n",
    "                    try: model.fit(X_train, y_train)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Training failed for {model_type}_v{i}: {e}\")\n",
    "                        continue\n",
    "                    for test_year_str, (X_test, y_test) in test_sets.items():\n",
    "                        if len(X_test) == 0: continue\n",
    "                        variant_name = f\"{model_type}_train_{train_years_str}_test_{test_year_str}_bin{bin_step}_pers{use_persistence}_v{i}\"\n",
    "                        evaluate_performance(model, variant_name, mz_feature_names, X_train, X_test, y_test)\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Chance\")\n",
    "            plt.legend(fontsize='x-small', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "            plt.title(f\"ROC Curves (Train: {train_years_str} / Bin: {bin_step} / Persist: {use_persistence})\")\n",
    "            plt.tight_layout(rect=[0, 0, 0.75, 1])\n",
    "            roc_filename = f\"roc_train_{train_years_str}_bin{bin_step}_pers{use_persistence}.png\"\n",
    "            plt.savefig(roc_filename, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"ROC Plot for scenario saved to {roc_filename}\")\n",
    "\n",
    "print(\"\\n\\n All processing complete. Saving final summaries.\")\n",
    "pd.DataFrame(summary).to_csv(\"master_auc_summary.csv\", index=False)\n",
    "pd.DataFrame(metrics_detailed).to_json(\"master_metrics_detailed.json\", orient=\"records\", indent=2)\n",
    "print(\"\\nDone: Master summary written to master_auc_summary.csv, full metrics in master_metrics_detailed.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116be4ba-3c20-4f69-b74b-4bf345ce91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# --- maldi_nn imports ---\n",
    "# Ensure maldi_nn is installed or its modules are accessible\n",
    "from maldi_nn.spectrum import (\n",
    "    SpectrumObject,\n",
    "    SequentialPreprocessor,\n",
    "    VarStabilizer,\n",
    "    Smoother,\n",
    "    BaselineCorrecter,\n",
    "    Trimmer\n",
    ")\n",
    "\n",
    "#==============================================================================\n",
    "# PART 1: DATA PREPARATION\n",
    "#==============================================================================\n",
    "\n",
    "class Aligner:\n",
    "    \"\"\"\n",
    "    A preprocessor step to align a spectrum's m/z axis to a reference peak list\n",
    "    using a robust RANSAC regressor.\n",
    "    \"\"\"\n",
    "    def __init__(self, reference_peaks, tolerance=500):\n",
    "        self.reference_peaks = np.array(reference_peaks)\n",
    "        self.tolerance = tolerance # tolerance in ppm\n",
    "\n",
    "    def __call__(self, spectrum: SpectrumObject) -> SpectrumObject:\n",
    "        peaks, _ = find_peaks(spectrum.intensity, height=0.05 * np.max(spectrum.intensity), prominence=0.01)\n",
    "        if len(peaks) < 5: return spectrum\n",
    "\n",
    "        sample_peak_mzs = spectrum.mz[peaks]\n",
    "        shifts = []\n",
    "        for ref_peak in self.reference_peaks:\n",
    "            tol_daltons = self.tolerance * ref_peak / 1e6\n",
    "            matches = np.where(np.abs(sample_peak_mzs - ref_peak) < tol_daltons)[0]\n",
    "            if len(matches) > 0:\n",
    "                best_match_idx = matches[np.argmin(np.abs(sample_peak_mzs[matches] - ref_peak))]\n",
    "                sample_peak_mz = sample_peak_mzs[best_match_idx]\n",
    "                shifts.append((sample_peak_mz, ref_peak - sample_peak_mz))\n",
    "\n",
    "        if len(shifts) < 3: return spectrum\n",
    "\n",
    "        shifts = np.array(shifts)\n",
    "        sample_mzs_for_fit, mz_shifts_for_fit = shifts[:, 0].reshape(-1, 1), shifts[:, 1]\n",
    "        \n",
    "        try:\n",
    "            ransac = RANSACRegressor(random_state=42).fit(sample_mzs_for_fit, mz_shifts_for_fit)\n",
    "            mz_correction = ransac.predict(spectrum.mz.reshape(-1, 1))\n",
    "            aligned_mz = spectrum.mz + mz_correction\n",
    "            return SpectrumObject(aligned_mz, spectrum.intensity)\n",
    "        except ValueError:\n",
    "            return spectrum\n",
    "\n",
    "def get_preprocessor(reference_file, mz_range=(2000, 12000)):\n",
    "    \"\"\"Initializes the preprocessing pipeline, including the RANSAC Aligner.\"\"\"\n",
    "    try:\n",
    "        ref_df = pd.read_csv(reference_file, sep='\\t', engine='python')\n",
    "        proton_mass = 1.007276 # Use precise proton mass for [M+H]+\n",
    "        reference_peaks = (ref_df['Mass'] + proton_mass).values\n",
    "        print(f\"Loaded {len(reference_peaks)} reference peaks for alignment.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not load or parse reference peak file '{reference_file}': {e}\")\n",
    "        return None\n",
    "\n",
    "    return SequentialPreprocessor(\n",
    "        VarStabilizer(method=\"sqrt\"),\n",
    "        Smoother(halfwindow=10),\n",
    "        BaselineCorrecter(method=\"SNIP\", snip_n_iter=20),\n",
    "        Aligner(reference_peaks=reference_peaks, tolerance=500),\n",
    "        Trimmer(*mz_range)\n",
    "    )\n",
    "\n",
    "def prepare_cnn_dataset(neg_dir, pos_dir, preprocessor, fixed_len=10000):\n",
    "    \"\"\"Loads, preprocesses, and resamples spectra by scanning directories.\"\"\"\n",
    "    print(\"--- Starting Data Preparation with RANSAC Alignment ---\")\n",
    "    samples, labels, years = [], [], []\n",
    "    data_sources = [(neg_dir, 0), (pos_dir, 1)]\n",
    "    \n",
    "    for data_dir, label in data_sources:\n",
    "        if not os.path.exists(data_dir):\n",
    "            print(f\"Directory not found, skipping: {data_dir}\")\n",
    "            continue\n",
    "        print(f\"Processing files from '{os.path.basename(data_dir)}' with label {label}...\")\n",
    "        for fname in os.listdir(data_dir):\n",
    "            if not fname.endswith(\".tsv\"): continue\n",
    "            file_path = os.path.join(data_dir, fname)\n",
    "            try:\n",
    "                year_match = re.match(r'^(\\d{4})', fname)\n",
    "                if not year_match: raise ValueError(\"Filename does not start with a 4-digit year.\")\n",
    "                year = int(year_match.group(1))\n",
    "                data = np.loadtxt(file_path, delimiter=\"\\t\")\n",
    "                s = SpectrumObject(data[:, 0], data[:, 1])\n",
    "                s_processed = preprocessor(s)\n",
    "                if s_processed is None or len(s_processed.mz) < 2: raise ValueError(\"Spectrum empty after preprocessing.\")\n",
    "                mz_min, mz_max = s_processed.mz.min(), s_processed.mz.max()\n",
    "                new_mz_grid = np.linspace(mz_min, mz_max, fixed_len)\n",
    "                vec = np.interp(new_mz_grid, s_processed.mz, s_processed.intensity).astype(np.float32)\n",
    "                if np.any(np.isnan(vec)) or np.any(np.isinf(vec)): raise ValueError(\"NaN or Inf in vector.\")\n",
    "                samples.append(vec)\n",
    "                labels.append(label)\n",
    "                years.append(year)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipped '{file_path}': {e}\")\n",
    "            \n",
    "    if not samples:\n",
    "        print(\"FATAL: No spectra were successfully processed.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"\\n Successfully processed {len(samples)} total spectra.\")\n",
    "    x_tensor = torch.tensor(np.array(samples), dtype=torch.float32).unsqueeze(1)\n",
    "    y_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    return TensorDataset(x_tensor, y_tensor), np.array(years)\n",
    "\n",
    "#==============================================================================\n",
    "# PART 2: CNN MODEL DEFINITIONS\n",
    "#==============================================================================\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_length=10000, num_classes=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 7, padding=3), nn.BatchNorm1d(8), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.25),\n",
    "            nn.Conv1d(8, 16, 5, padding=2), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.35),\n",
    "            nn.Conv1d(16, 32, 3, padding=1), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.5)\n",
    "        )\n",
    "        with torch.no_grad(): self.flattened_size = self.features(torch.zeros(1, 1, input_length)).numel()\n",
    "        self.classifier = nn.Sequential(nn.Linear(self.flattened_size, 256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, num_classes))\n",
    "    def forward(self, x):\n",
    "        x = self.features(x); x = x.view(x.size(0), -1); return self.classifier(x)\n",
    "\n",
    "class MaldiCNN(nn.Module):\n",
    "    def __init__(self, input_length=10000, num_classes=2):\n",
    "        super(MaldiCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 7, padding=3), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.2),\n",
    "            nn.Conv1d(16, 32, 5, padding=2), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.3),\n",
    "            nn.Conv1d(32, 64, 3, padding=1), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.4),\n",
    "            nn.Conv1d(64, 128, 3, padding=1), nn.BatchNorm1d(128), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.5)\n",
    "        )\n",
    "        with torch.no_grad(): self.flattened_size = self.features(torch.zeros(1, 1, input_length)).numel()\n",
    "        self.classifier = nn.Sequential(nn.Linear(self.flattened_size, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, num_classes))\n",
    "    def forward(self, x):\n",
    "        x = self.features(x); x = x.view(x.size(0), -1); return self.classifier(x)\n",
    "\n",
    "class DeeperMaldiCNN(nn.Module):\n",
    "    def __init__(self, input_length=10000, num_classes=2):\n",
    "        super(DeeperMaldiCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 9, padding=4), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.2),\n",
    "            nn.Conv1d(16, 32, 7, padding=3), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.3),\n",
    "            nn.Conv1d(32, 64, 5, padding=2), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.4),\n",
    "            nn.Conv1d(64, 128, 3, padding=1), nn.BatchNorm1d(128), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.5),\n",
    "            nn.Conv1d(128, 256, 3, padding=1), nn.BatchNorm1d(256), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.5)\n",
    "        )\n",
    "        with torch.no_grad(): self.flattened_size = self.features(torch.zeros(1, 1, input_length)).numel()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 1024), nn.ReLU(), nn.Dropout(0.5), nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x); x = x.view(x.size(0), -1); return self.classifier(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, 3, stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels: self.shortcut = nn.Sequential(nn.Conv1d(in_channels, out_channels, 1, stride, bias=False), nn.BatchNorm1d(out_channels))\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x); out = self.relu(self.bn1(self.conv1(x))); out = self.bn2(self.conv2(out)); out += identity; return self.relu(out)\n",
    "\n",
    "class MaldiResNet(nn.Module):\n",
    "    def __init__(self, input_length=10000, num_classes=2):\n",
    "        super(MaldiResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv1 = nn.Conv1d(1, 16, 7, 2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(16); self.relu = nn.ReLU(inplace=True); self.maxpool = nn.MaxPool1d(3, 2, padding=1)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1); self.layer2 = self._make_layer(32, 2, stride=2); self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1); self.fc = nn.Linear(64, num_classes)\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides, layers = [stride] + [1]*(num_blocks-1), []\n",
    "        for s in strides: layers.append(ResidualBlock(self.in_channels, out_channels, s)); self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.bn1(self.conv1(x)))); x = self.layer1(x); x = self.layer2(x); x = self.layer3(x)\n",
    "        x = self.avgpool(x); x = x.view(x.size(0), -1); return self.fc(x)\n",
    "\n",
    "class DeeperMaldiResNet(nn.Module):\n",
    "    def __init__(self, input_length=10000, num_classes=2):\n",
    "        super(DeeperMaldiResNet, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        self.conv1 = nn.Conv1d(1, 32, 7, 2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32); self.relu = nn.ReLU(inplace=True); self.maxpool = nn.MaxPool1d(3, 2, padding=1)\n",
    "        self.layer1 = self._make_layer(32, 3, stride=1); self.layer2 = self._make_layer(64, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 6, stride=2); self.layer4 = self._make_layer(256, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1); self.fc = nn.Linear(256, num_classes)\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides, layers = [stride] + [1]*(num_blocks-1), []\n",
    "        for s in strides: layers.append(ResidualBlock(self.in_channels, out_channels, s)); self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.bn1(self.conv1(x)))); x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
    "        x = self.avgpool(x); x = x.view(x.size(0), -1); return self.fc(x)\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# 3. MAIN EXECUTION BLOCK\n",
    "#==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    neg_dir = r\"Y:\\\\test_set\\\\allspectra\\\\neg_spectra\\\\neg_tsv\"\n",
    "    pos_dir = r\"Y:\\\\test_set\\\\allspectra\\\\pos_spectra\\\\pos_tsv\"\n",
    "    ribo_masslist = r\"Y:\\\\test_set\\\\ribo_Saureus.tsv\"\n",
    "    MZ_RANGE = (2000, 12000)\n",
    "    VECTOR_LENGTH = 10000\n",
    "    BEST_MODEL_PATH = 'best_model.pth' # Assumes a pre-trained model exists\n",
    "    \n",
    "    # --- STEP 1: DATA PREPARATION ---\n",
    "    preprocessor = get_preprocessor(ribo_masslist, mz_range=MZ_RANGE)\n",
    "    if preprocessor:\n",
    "        full_dataset, years_array = prepare_cnn_dataset(\n",
    "            neg_dir=neg_dir, pos_dir=pos_dir, preprocessor=preprocessor, fixed_len=VECTOR_LENGTH\n",
    "        )\n",
    "        if full_dataset:\n",
    "            print(\"\\n--- Data Shapes ---\")\n",
    "            print(f\"Features (X) tensor shape: {full_dataset.tensors[0].shape}\")\n",
    "            print(f\"Labels   (y) tensor shape: {full_dataset.tensors[1].shape}\")\n",
    "            print(f\"Unique years found: {np.unique(years_array)}\")\n",
    "            print(\"\\n Data preparation complete.\")\n",
    "            \n",
    "            # --- STEP 2: SHAP ANALYSIS ---\n",
    "            print(\"\\n\\n\" + \"=\"*25 + \" SHAP EXPLAINABILITY ANALYSIS \" + \"=\"*25)\n",
    "            if not os.path.exists(BEST_MODEL_PATH):\n",
    "                print(f\"Could not perform SHAP analysis: The model file '{BEST_MODEL_PATH}' was not found.\")\n",
    "                print(\"Please train a model and save it to this path first.\")\n",
    "            else:\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model_zoo = {\n",
    "                    \"SimpleCNN\": SimpleCNN, \"MaldiCNN\": MaldiCNN, \"DeeperMaldiCNN\": DeeperMaldiCNN,\n",
    "                    \"MaldiResNet\": MaldiResNet, \"DeeperMaldiResNet\": DeeperMaldiResNet\n",
    "                }\n",
    "                \n",
    "                print(f\"Loading best model from '{BEST_MODEL_PATH}'...\")\n",
    "                checkpoint = torch.load(BEST_MODEL_PATH, map_location=device)\n",
    "                model_name = checkpoint.get('model_name')\n",
    "                if not model_name or model_name not in model_zoo:\n",
    "                    print(f\"Error: Model name '{model_name}' not found in checkpoint or model zoo.\")\n",
    "                else:\n",
    "                    model_class = model_zoo[model_name]\n",
    "                    num_classes = len(torch.unique(full_dataset.tensors[1]))\n",
    "                    model = model_class(VECTOR_LENGTH, num_classes).to(device)\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    model.eval()\n",
    "\n",
    "                    val_indices = checkpoint.get('val_indices', [])\n",
    "                    if not val_indices:\n",
    "                        print(\"Error: 'val_indices' not found in checkpoint. Cannot select data for explanation.\")\n",
    "                    else:\n",
    "                        test_subset = Subset(full_dataset, val_indices)\n",
    "                        background_subset = Subset(full_dataset, checkpoint.get('train_indices', [])[:100])\n",
    "                        \n",
    "                        test_loader = DataLoader(test_subset, batch_size=64)\n",
    "                        background_loader = DataLoader(background_subset, batch_size=100)\n",
    "                        \n",
    "                        test_tensors, _ = next(iter(test_loader))\n",
    "                        background_tensors, _ = next(iter(background_loader))\n",
    "                        \n",
    "                        print(\"Initializing DeepExplainer...\")\n",
    "                        explainer = shap.DeepExplainer(model, background_tensors.to(device))\n",
    "                        print(f\"Calculating SHAP values for {test_tensors.shape[0]} test samples...\")\n",
    "                        \n",
    "                        shap_values_raw = explainer.shap_values(test_tensors.to(device))\n",
    "                        \n",
    "                        # shap_values_raw for binary classification with 1D Conv is a list of 2 arrays of shape (N, 1, L)\n",
    "                        # We want the values for the positive class (class 1)\n",
    "                        shap_values_class1 = shap_values_raw[1].squeeze(1) # Squeeze the channel dimension\n",
    "\n",
    "                        feature_names = [f\"m/z {mz:.1f}\" for mz in np.linspace(MZ_RANGE[0], MZ_RANGE[1], VECTOR_LENGTH)]\n",
    "                        \n",
    "                        print(\"\\nGenerating SHAP summary plot (beeswarm)...\")\n",
    "                        shap.summary_plot(shap_values_class1, features=test_tensors.squeeze(1).cpu().numpy(), feature_names=feature_names, max_display=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e206fd-2f72-459d-8203-77181535e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pandas as pd\n",
    "import shap\n",
    "import os\n",
    "try:\n",
    "    from scipy.signal import find_peaks\n",
    "except ImportError:\n",
    "    print(\"Please install scipy for peak detection: pip install scipy\")\n",
    "    # Define a dummy function if scipy is not available to avoid crashing the script\n",
    "    def find_peaks(*args, **kwargs):\n",
    "        print(\"WARNING: scipy.signal.find_peaks is not available. Peak detection will be skipped.\")\n",
    "        return np.array([]), {}\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# 1. RE-DEFINE MODEL ZOO (This must be consistent with the training script)\n",
    "#==============================================================================\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_length, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, 7, padding=3), nn.BatchNorm1d(8), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.25),\n",
    "            nn.Conv1d(8, 16, 5, padding=2), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.35),\n",
    "            nn.Conv1d(16, 32, 3, padding=1), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.5)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.flattened_size = self.features(torch.zeros(1, 1, input_length)).numel()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 256), nn.ReLU(), nn.Dropout(0.5), nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class MaldiCNN(nn.Module):\n",
    "    def __init__(self, input_length, num_classes):\n",
    "        super(MaldiCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 7, padding=3), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.2),\n",
    "            nn.Conv1d(16, 32, 5, padding=2), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.3),\n",
    "            nn.Conv1d(32, 64, 3, padding=1), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.4),\n",
    "            nn.Conv1d(64, 128, 3, padding=1), nn.BatchNorm1d(128), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.5)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.flattened_size = self.features(torch.zeros(1, 1, input_length)).numel()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class DeeperMaldiCNN(nn.Module):\n",
    "    def __init__(self, input_length, num_classes):\n",
    "        super(DeeperMaldiCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, 9, padding=4), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.2),\n",
    "            nn.Conv1d(16, 32, 7, padding=3), nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.3),\n",
    "            nn.Conv1d(32, 64, 5, padding=2), nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.4),\n",
    "            nn.Conv1d(64, 128, 3, padding=1), nn.BatchNorm1d(128), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.5),\n",
    "            nn.Conv1d(128, 256, 3, padding=1), nn.BatchNorm1d(256), nn.ReLU(), nn.MaxPool1d(2), nn.Dropout(0.5)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.flattened_size = self.features(torch.zeros(1, 1, input_length)).numel()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 1024), nn.ReLU(), nn.Dropout(0.5), nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, 3, stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(nn.Conv1d(in_channels, out_channels, 1, stride, bias=False), nn.BatchNorm1d(out_channels))\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class MaldiResNet(nn.Module):\n",
    "    def __init__(self, input_length, num_classes):\n",
    "        super(MaldiResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv1 = nn.Conv1d(1, 16, 7, 2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(3, 2, padding=1)\n",
    "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class DeeperMaldiResNet(nn.Module):\n",
    "    def __init__(self, input_length, num_classes):\n",
    "        super(DeeperMaldiResNet, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        self.conv1 = nn.Conv1d(1, 32, 7, 2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(3, 2, padding=1)\n",
    "        self.layer1 = self._make_layer(32, 3, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.layer1(x); x = self.layer2(x); x = self.layer3(x); x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "#==============================================================================\n",
    "# 2. SHAP ANALYSIS EXECUTION (Refactored for Robustness)\n",
    "#==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    BEST_MODEL_PATH = 'best_model.pth'\n",
    "    VECTOR_LENGTH = 10000\n",
    "    SHAP_VALUES_PATH = 'shap_values.csv'\n",
    "    FEATURE_VALUES_PATH = 'feature_values_for_shap.csv'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model_zoo = {\n",
    "        \"SimpleCNN\": SimpleCNN,\n",
    "        \"MaldiCNN_4_Layer\": MaldiCNN,\n",
    "        \"DeeperMaldiCNN_5_Layer\": DeeperMaldiCNN,\n",
    "        \"MaldiResNet_Shazam\": MaldiResNet,\n",
    "        \"DeeperMaldiResNet\": DeeperMaldiResNet\n",
    "    }\n",
    "\n",
    "    print(\"\\n\\n\" + \"=\"*25 + \" SHAP EXPLAINABILITY ANALYSIS \" + \"=\"*25)\n",
    "\n",
    "    if 'full_dataset' not in locals() or 'years_array' not in locals():\n",
    "        print(\"Prerequisite 'full_dataset' or 'years_array' not found. Please run Script 1 first.\")\n",
    "    elif os.path.exists(BEST_MODEL_PATH):\n",
    "        print(f\"Loading best model from '{BEST_MODEL_PATH}'...\")\n",
    "        \n",
    "        checkpoint = torch.load(BEST_MODEL_PATH, map_location=device, weights_only=False)\n",
    "        model_name = checkpoint['model_name']\n",
    "        model_class = model_zoo.get(model_name)\n",
    "        \n",
    "        if model_class is None:\n",
    "            print(f\"Error: Model name '{model_name}' from saved file is not defined in the model_zoo.\")\n",
    "        else:\n",
    "            if not os.path.exists(SHAP_VALUES_PATH):\n",
    "                # ... (Part 1 code to calculate SHAP would go here if needed) ...\n",
    "                pass\n",
    "            \n",
    "            # --- Load data for plotting ---\n",
    "            print(\"\\n--- Loading data for analysis ---\")\n",
    "            shap_df = pd.read_csv(SHAP_VALUES_PATH, index_col=0)\n",
    "            feature_values_df = pd.read_csv(FEATURE_VALUES_PATH, index_col=0)\n",
    "            \n",
    "            # --- Part 5: Advanced Peak Analysis and Mirror Plot (MODIFIED) ---\n",
    "            try:\n",
    "                print(\"\\n\" + \"=\"*25 + \" PEAK ANALYSIS & PLOTTING \" + \"=\"*25)\n",
    "                \n",
    "                mean_shaps = shap_df.mean(axis=1)\n",
    "                mz_values = np.array([float(s.split(' ')[1]) for s in shap_df.index])\n",
    "\n",
    "                print(\"Finding positive and negative peaks...\")\n",
    "                pos_peaks_idx, pos_properties = find_peaks(mean_shaps.values, prominence=0.005, distance=50)\n",
    "                neg_peaks_idx, neg_properties = find_peaks(-mean_shaps.values, prominence=0.005, distance=50)\n",
    "                print(f\"Found {len(pos_peaks_idx)} positive peaks and {len(neg_peaks_idx)} negative peaks.\")\n",
    "\n",
    "                # --- Mirror Plot ---\n",
    "                # (This can be kept or removed, as the separate plots below are more detailed)\n",
    "                \n",
    "                # --- Positive Plot and Table ---\n",
    "                print(\"\\n--- Generating Positive Impact Plot and Table ---\")\n",
    "                plt.figure(figsize=(20, 6))\n",
    "                plt.plot(mz_values, mean_shaps.where(mean_shaps > 0), color='red', alpha=0.7)\n",
    "                plt.scatter(mz_values[pos_peaks_idx], mean_shaps.values[pos_peaks_idx], color='maroon', s=50, zorder=5, marker='X')\n",
    "                top_pos_indices = np.argsort(pos_properties['prominences'])[-10:]\n",
    "                for i in pos_peaks_idx[top_pos_indices]:\n",
    "                    plt.text(mz_values[i], mean_shaps.values[i], f' {mz_values[i]:.1f}', verticalalignment='bottom', fontsize=9)\n",
    "                plt.title(f'Positive SHAP Impact (-> Class 1) for {model_name}', fontsize=16)\n",
    "                plt.xlabel(\"m/z\", fontsize=12)\n",
    "                plt.ylabel(\"Mean SHAP Value\", fontsize=12)\n",
    "                plt.grid(True, linestyle=':', alpha=0.6)\n",
    "                plt.tight_layout()\n",
    "                save_path = f\"{model_name}_shap_positive_peaks_plot.png\"\n",
    "                plt.savefig(save_path, dpi=150)\n",
    "                print(f\"Saved Positive Peaks Plot to: {save_path}\")\n",
    "                plt.show()\n",
    "\n",
    "                # Create, print, and save the corresponding table for ALL positive peaks\n",
    "                pos_peaks_df = pd.DataFrame({\n",
    "                    'm/z': mz_values[pos_peaks_idx],\n",
    "                    'Mean SHAP Value': mean_shaps.values[pos_peaks_idx],\n",
    "                    'Prominence': pos_properties['prominences']\n",
    "                }).sort_values(by='Prominence', ascending=False).reset_index(drop=True)\n",
    "                \n",
    "                pos_table_path = 'positive_peaks_table.csv'\n",
    "                pos_peaks_df.to_csv(pos_table_path, index=False)\n",
    "                print(f\"Table of all {len(pos_peaks_df)} positive peaks saved to '{pos_table_path}'\")\n",
    "                print(\"--- Table for Positive Peaks (Sorted by Prominence) ---\")\n",
    "                print(pos_peaks_df.to_string())\n",
    "\n",
    "\n",
    "                # --- Negative Plot and Table ---\n",
    "                print(\"\\n--- Generating Negative Impact Plot and Table ---\")\n",
    "                plt.figure(figsize=(20, 6))\n",
    "                plt.plot(mz_values, mean_shaps.where(mean_shaps < 0), color='royalblue', alpha=0.7)\n",
    "                plt.scatter(mz_values[neg_peaks_idx], mean_shaps.values[neg_peaks_idx], color='navy', s=50, zorder=5, marker='X')\n",
    "                top_neg_indices = np.argsort(neg_properties['prominences'])[-10:]\n",
    "                for i in neg_peaks_idx[top_neg_indices]:\n",
    "                    plt.text(mz_values[i], mean_shaps.values[i], f' {mz_values[i]:.1f}', verticalalignment='top', fontsize=9)\n",
    "                plt.title(f'Negative SHAP Impact (-> Class 0) for {model_name}', fontsize=16)\n",
    "                plt.xlabel(\"m/z\", fontsize=12)\n",
    "                plt.ylabel(\"Mean SHAP Value\", fontsize=12)\n",
    "                plt.grid(True, linestyle=':', alpha=0.6)\n",
    "                plt.tight_layout()\n",
    "                save_path = f\"{model_name}_shap_negative_peaks_plot.png\"\n",
    "                plt.savefig(save_path, dpi=150)\n",
    "                print(f\"Saved Negative Peaks Plot to: {save_path}\")\n",
    "                plt.show()\n",
    "\n",
    "                # Create, print, and save the corresponding table for ALL negative peaks\n",
    "                neg_peaks_df = pd.DataFrame({\n",
    "                    'm/z': mz_values[neg_peaks_idx],\n",
    "                    'Mean SHAP Value': mean_shaps.values[neg_peaks_idx],\n",
    "                    'Prominence': neg_properties['prominences']\n",
    "                }).sort_values(by='Prominence', ascending=False).reset_index(drop=True)\n",
    "\n",
    "                neg_table_path = 'negative_peaks_table.csv'\n",
    "                neg_peaks_df.to_csv(neg_table_path, index=False)\n",
    "                print(f\"Table of all {len(neg_peaks_df)} negative peaks saved to '{neg_table_path}'\")\n",
    "                print(\"--- Table for Negative Peaks (Sorted by Prominence) ---\")\n",
    "                print(neg_peaks_df.to_string())\n",
    "\n",
    "            except NameError:\n",
    "                 print(\"Could not generate peak analysis plots because `find_peaks` is not available. Is `scipy` installed?\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not generate peak analysis plots: {e}\")\n",
    "            \n",
    "            # --- Part 6: FINAL SYNTHESIS: COMBINED PLOT AND PEAK TABLE ---\n",
    "            try:\n",
    "                print(\"\\n\" + \"=\"*25 + \" FINAL SYNTHESIS \" + \"=\"*25)\n",
    "\n",
    "                # --- Combined Summary Table ---\n",
    "                print(\"\\nCreating combined peak summary table...\")\n",
    "                all_peaks_df = pd.concat([\n",
    "                    pos_peaks_df.assign(**{'Class Impact': '-> Class 1'}),\n",
    "                    neg_peaks_df.assign(**{'Class Impact': '-> Class 0'})\n",
    "                ]).sort_values(by='Prominence', ascending=False).reset_index(drop=True)\n",
    "                \n",
    "                peak_table_path = 'identified_peaks_summary.csv'\n",
    "                all_peaks_df.to_csv(peak_table_path, index=False)\n",
    "                print(f\"Combined peak summary table saved to '{peak_table_path}'\")\n",
    "                print(\"Top 15 Most Prominent Peaks (Combined):\")\n",
    "                print(all_peaks_df.head(15).to_string())\n",
    "\n",
    "                # --- Combined Overlay Plot ---\n",
    "                print(\"\\nGenerating Combined Spectrum vs. SHAP Plot...\")\n",
    "                all_labels = full_dataset.tensors[1].cpu().numpy()\n",
    "                val_indices = checkpoint['val_indices']\n",
    "                val_labels = all_labels[val_indices]\n",
    "                \n",
    "                mean_class1_spectrum = feature_values_df.iloc[:, val_labels == 1].mean(axis=1)\n",
    "                mean_class0_spectrum = feature_values_df.iloc[:, val_labels == 0].mean(axis=1)\n",
    "                \n",
    "                fig, ax1 = plt.subplots(figsize=(20, 8))\n",
    "                p1, = ax1.plot(mz_values, mean_shaps.where(mean_shaps > 0), color='red', alpha=0.7, label='Positive SHAP Impact')\n",
    "                p2, = ax1.plot(mz_values, mean_shaps.where(mean_shaps < 0), color='royalblue', alpha=0.7, label='Negative SHAP Impact')\n",
    "                ax1.set_xlabel('m/z', fontsize=12)\n",
    "                ax1.set_ylabel('Mean SHAP Value', color='black', fontsize=12)\n",
    "                ax1.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "                ax2 = ax1.twinx()\n",
    "                p3 = ax2.fill_between(mz_values, 0, mean_class1_spectrum, color='salmon', alpha=0.3, label='Mean Spectrum (Class 1)')\n",
    "                p4 = ax2.fill_between(mz_values, 0, mean_class0_spectrum, color='skyblue', alpha=0.3, label='Mean Spectrum (Class 0)')\n",
    "                ax2.set_ylabel('Mean Intensity', color='gray', fontsize=12)\n",
    "                \n",
    "                plt.title(f'Combined Plot: Mean Spectra vs. Mean SHAP Impact for {model_name}', fontsize=16)\n",
    "                ax1.set_xlim(2000, 12000)\n",
    "                ax1.legend([p1, p2, p3, p4], [p.get_label() for p in [p1, p2, p3, p4]], loc='upper right')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                save_path = f\"{model_name}_shap_combined_overlay_plot.png\"\n",
    "                plt.savefig(save_path, dpi=150)\n",
    "                print(f\"Saved Combined Overlay Plot to: {save_path}\")\n",
    "                plt.show()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not generate final synthesis plots: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Could not perform SHAP analysis: The model file '{BEST_MODEL_PATH}' was not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
